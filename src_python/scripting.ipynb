{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# An instance of your model.\n",
    "model = torchvision.models.resnet18()\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save(\"traced_script_module.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.jit._trace.TopLevelTracedModule'>\n"
     ]
    }
   ],
   "source": [
    "print(type(traced_script_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = traced_script_module(torch.ones(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4811,  0.6819, -0.0026, -0.1308,  0.3430], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output[0,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.rand(N, M))\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.sum() > 0:\n",
    "          output = self.weight.mv(input)\n",
    "        else:\n",
    "          output = self.weight + input\n",
    "        return output\n",
    "\n",
    "my_module = MyModule(10,20)\n",
    "sm = torch.jit.script(my_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.save(\"my_module.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8556,  0.9050,  0.1528,  0.2607],\n",
       "         [ 0.8860,  0.9079, -0.1406,  0.8220],\n",
       "         [ 0.7556,  0.8808, -0.4993, -0.0300]], grad_fn=<TanhBackward>),\n",
       " tensor([[ 0.8556,  0.9050,  0.1528,  0.2607],\n",
       "         [ 0.8860,  0.9079, -0.1406,  0.8220],\n",
       "         [ 0.7556,  0.8808, -0.4993, -0.0300]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8556,  0.9050,  0.1528,  0.2607],\n",
       "         [ 0.8860,  0.9079, -0.1406,  0.8220],\n",
       "         [ 0.7556,  0.8808, -0.4993, -0.0300]], grad_fn=<TanhBackward>),\n",
       " tensor([[ 0.8556,  0.9050,  0.1528,  0.2607],\n",
       "         [ 0.8860,  0.9079, -0.1406,  0.8220],\n",
       "         [ 0.7556,  0.8808, -0.4993, -0.0300]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cell(x,h)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.MyCell,\n",
      "      %input : Float(3:4, 4:1, requires_grad=0, device=cpu),\n",
      "      %h : Float(3:4, 4:1, requires_grad=0, device=cpu)):\n",
      "  %19 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n",
      "  %21 : Tensor = prim::CallMethod[name=\"forward\"](%19, %input)\n",
      "  %12 : int = prim::Constant[value=1]() # <ipython-input-5-1f6e08af67d0>:7:0\n",
      "  %13 : Float(3:4, 4:1, requires_grad=1, device=cpu) = aten::add(%21, %h, %12) # <ipython-input-5-1f6e08af67d0>:7:0\n",
      "  %14 : Float(3:4, 4:1, requires_grad=1, device=cpu) = aten::tanh(%13) # <ipython-input-5-1f6e08af67d0>:7:0\n",
      "  %15 : (Float(3:4, 4:1, requires_grad=1, device=cpu), Float(3:4, 4:1, requires_grad=1, device=cpu)) = prim::TupleConstruct(%14, %14)\n",
      "  return (%15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = torch.add((self.linear).forward(input, ), h, alpha=1)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "from babel.numbers import parse_decimal\n",
    "x = '6.3'\n",
    "val = float(parse_decimal('8', locale='en_US'))\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%actual_input_1 : Float(10:150528, 3:50176, 224:224, 224:1, requires_grad=0, device=cpu),\n",
      "      %learned_0 : Float(64:363, 3:121, 11:11, 11:1, requires_grad=1, device=cpu),\n",
      "      %learned_1 : Float(64:1, requires_grad=1, device=cpu),\n",
      "      %learned_2 : Float(192:1600, 64:25, 5:5, 5:1, requires_grad=1, device=cpu),\n",
      "      %learned_3 : Float(192:1, requires_grad=1, device=cpu),\n",
      "      %learned_4 : Float(384:1728, 192:9, 3:3, 3:1, requires_grad=1, device=cpu),\n",
      "      %learned_5 : Float(384:1, requires_grad=1, device=cpu),\n",
      "      %learned_6 : Float(256:3456, 384:9, 3:3, 3:1, requires_grad=1, device=cpu),\n",
      "      %learned_7 : Float(256:1, requires_grad=1, device=cpu),\n",
      "      %learned_8 : Float(256:2304, 256:9, 3:3, 3:1, requires_grad=1, device=cpu),\n",
      "      %learned_9 : Float(256:1, requires_grad=1, device=cpu),\n",
      "      %learned_10 : Float(4096:9216, 9216:1, requires_grad=1, device=cpu),\n",
      "      %learned_11 : Float(4096:1, requires_grad=1, device=cpu),\n",
      "      %learned_12 : Float(4096:4096, 4096:1, requires_grad=1, device=cpu),\n",
      "      %learned_13 : Float(4096:1, requires_grad=1, device=cpu),\n",
      "      %learned_14 : Float(1000:4096, 4096:1, requires_grad=1, device=cpu),\n",
      "      %learned_15 : Float(1000:1, requires_grad=1, device=cpu)):\n",
      "  %17 : Float(10:193600, 64:3025, 55:55, 55:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1) # /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %18 : Float(10:193600, 64:3025, 55:55, 55:1, requires_grad=1, device=cpu) = onnx::Relu(%17) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %19 : Float(10:46656, 64:729, 27:27, 27:1, requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:586:0\n",
      "  %20 : Float(10:139968, 192:729, 27:27, 27:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %learned_2, %learned_3) # /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %21 : Float(10:139968, 192:729, 27:27, 27:1, requires_grad=1, device=cpu) = onnx::Relu(%20) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %22 : Float(10:32448, 192:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:586:0\n",
      "  %23 : Float(10:64896, 384:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %learned_4, %learned_5) # /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %24 : Float(10:64896, 384:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Relu(%23) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %25 : Float(10:43264, 256:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %learned_6, %learned_7) # /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %26 : Float(10:43264, 256:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Relu(%25) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %27 : Float(10:43264, 256:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %learned_8, %learned_9) # /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py:420:0\n",
      "  %28 : Float(10:43264, 256:169, 13:13, 13:1, requires_grad=1, device=cpu) = onnx::Relu(%27) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %29 : Float(10:9216, 256:36, 6:6, 6:1, requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:586:0\n",
      "  %30 : Float(10:9216, 256:36, 6:6, 6:1, requires_grad=1, device=cpu) = onnx::AveragePool[kernel_shape=[1, 1], strides=[1, 1]](%29) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:936:0\n",
      "  %31 : Float(10:9216, 9216:1, requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%30) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:983:0\n",
      "  %32 : Float(10:4096, 4096:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%31, %learned_10, %learned_11) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1690:0\n",
      "  %33 : Float(10:4096, 4096:1, requires_grad=1, device=cpu) = onnx::Relu(%32) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:983:0\n",
      "  %34 : Float(10:4096, 4096:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%33, %learned_12, %learned_13) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1690:0\n",
      "  %35 : Float(10:4096, 4096:1, requires_grad=1, device=cpu) = onnx::Relu(%34) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1134:0\n",
      "  %output1 : Float(10:1000, 1000:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%35, %learned_14, %learned_15) # /usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1690:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(10, 3, 224, 224, device='cpu')\n",
    "model = torchvision.models.alexnet(pretrained=True).cpu()\n",
    "\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. Setting these does not change the semantics\n",
    "# of the graph; it is only for readability.\n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs (i.e.\n",
    "# the values you would pass to the forward() method) followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run failed to load cuda module:No module named 'caffe2.python.caffe2_pybind11_state_gpu',and AMD hip module:No module named 'caffe2.python.caffe2_pybind11_state_hip'.Will run in CPU only mode.\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'AlexNet' object has no attribute 'SerializeToString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ac9e5c8d840d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# or \"CPU\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# For the Caffe2 backend:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     rep.predict_net is the Caffe2 protobuf for the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/caffe2/python/onnx/backend.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(cls, model, device, raw_values_dict, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m         '''\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no_check_UNSAFE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCaffe2Backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mopset_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopset_import\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/onnx/backend/base.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(cls, model, device, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 ):  # type: (...) -> Optional[BackendRep]\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# TODO Remove Optional from return type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/onnx/checker.py\u001b[0m in \u001b[0;36mcheck_model\u001b[0;34m(model, full_check)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'AlexNet' object has no attribute 'SerializeToString'"
     ]
    }
   ],
   "source": [
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model, device=\"CPU\") # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "outputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15288618 -1.7744154  -1.772217   ... -1.2685202  -1.1067306\n",
      "   1.457113  ]\n",
      " [-0.06498257 -0.9908374  -1.1324918  ... -0.95935225 -0.97419715\n",
      "   1.2393706 ]\n",
      " [-0.05368517 -1.7085843  -1.1514349  ... -1.3531338  -0.936008\n",
      "   1.0077317 ]\n",
      " ...\n",
      " [-0.09625517 -1.5160254  -1.2915084  ... -1.3142797  -0.89416385\n",
      "   0.889783  ]\n",
      " [ 0.09142481 -1.2674376  -1.1974701  ... -0.8329861  -0.82525307\n",
      "   1.1199676 ]\n",
      " [ 0.04747842 -1.453126   -1.6183759  ... -1.3628887  -0.90458643\n",
      "   0.7608837 ]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession('alexnet.onnx')\n",
    "\n",
    "outputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n",
    "\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.alexnet.AlexNet'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input_data : Long(2:3, 3:1, requires_grad=0, device=cpu)):\n",
      "  %2 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n",
      "  %3 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Add(%input_data, %2)\n",
      "  %4 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %5 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Add(%3, %4)\n",
      "  %6 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={2}]()\n",
      "  %7 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Add(%5, %6)\n",
      "  %8 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n",
      "  %9 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Add(%7, %8)\n",
      "  %10 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={4}]()\n",
      "  %11 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Add(%9, %10)\n",
      "  return (%11)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Trace-based only\n",
    "\n",
    "class LoopModel(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        for i in range(y):\n",
    "            x = x + i\n",
    "        return x\n",
    "\n",
    "loopmodel = LoopModel()\n",
    "dummy_input = torch.ones(2, 3, dtype=torch.long)\n",
    "loop_count = torch.tensor(5, dtype=torch.long)\n",
    "\n",
    "torch.onnx.export(loopmodel, (dummy_input, loop_count), 'loop.onnx', verbose=True,input_names=['input_data', 'loop_range'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[11, 11, 11],\n",
      "       [11, 11, 11]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort_sess = ort.InferenceSession('loop.onnx')\n",
    "outputs = ort_sess.run(None, {'input_data': dummy_input.numpy()})\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input_data : Long(2:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %loop_range : Long(requires_grad=0, device=cpu),\n",
      "      %10 : Bool(requires_grad=0, device=cpu)):\n",
      "  %2 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %4 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Loop(%loop_range, %10, %input_data) # <ipython-input-22-a4f9461791d8>:3:4\n",
      "    block0(%i.1 : Long(device=cpu), %cond : bool, %x.6 : Long(2:3, 3:1, requires_grad=0, device=cpu)):\n",
      "      %8 : LongTensor = onnx::Add(%x.6, %i.1) # <ipython-input-22-a4f9461791d8>:4:12\n",
      "      %9 : bool = onnx::Cast[to=9](%2)\n",
      "      -> (%9, %8)\n",
      "  return (%4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def loop(x, y):\n",
    "    for i in range(int(y)):\n",
    "        x = x + i\n",
    "    return x\n",
    "\n",
    "class LoopModel2(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return loop(x, y)\n",
    "\n",
    "lpmodel = LoopModel2()\n",
    "\n",
    "dummy_input = torch.ones(2, 3, dtype=torch.long)\n",
    "loop_count = torch.tensor(5, dtype=torch.long)\n",
    "torch.onnx.export(lpmodel, (dummy_input, loop_count), 'lploop.onnx', verbose=True,\n",
    "                  input_names=['input_data', 'loop_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[11, 11, 11],\n",
      "       [11, 11, 11]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort_sess = ort.InferenceSession('lploop.onnx')\n",
    "outputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),'loop_range':np.array(5).astype(np.int64)})\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpmodel_script= torch.jit.script(lpmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    y: Tensor) -> Tensor:\n",
      "  x0 = x\n",
      "  for i in range(int(y)):\n",
      "    x0 = torch.add(x0, i, 1)\n",
      "  return x0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lpmodel_script.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input_data : Long(2:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %loop_range : Long(requires_grad=0, device=cpu),\n",
      "      %10 : Bool(requires_grad=0, device=cpu)):\n",
      "  %2 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %4 : Long(2:3, 3:1, requires_grad=0, device=cpu) = onnx::Loop(%loop_range, %10, %input_data) # <ipython-input-22-a4f9461791d8>:3:4\n",
      "    block0(%i.1 : Long(device=cpu), %cond : bool, %x.6 : Long(2:3, 3:1, requires_grad=0, device=cpu)):\n",
      "      %8 : LongTensor = onnx::Add(%x.6, %i.1) # <ipython-input-22-a4f9461791d8>:4:12\n",
      "      %9 : bool = onnx::Cast[to=9](%2)\n",
      "      -> (%9, %8)\n",
      "  return (%4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = lpmodel_script(dummy_input, loop_count)\n",
    "torch.onnx.export(lpmodel_script, (dummy_input, loop_count), 'lploop_script.onnx', verbose=True,\n",
    "                  input_names=['input_data', 'loop_range'],example_outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[37, 37, 37],\n",
      "       [37, 37, 37]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort_sess = ort.InferenceSession('lploop_script.onnx')\n",
    "outputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),'loop_range':np.array(9).astype(np.int64)})\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
