{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型和梯度下降\n",
    "这是神经网络的第一课，我们会学习一个非常简单的模型，线性回归，同时也会学习一个优化算法-梯度下降法，对这个模型进行优化。线性回归是监督学习里面一个非常简单的模型，同时梯度下降也是深度学习中应用最广的优化算法，我们将从这里开始我们的深度学习之旅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一元线性回归\n",
    "一元线性模型非常简单，假设我们有变量 $x_i$ 和目标 $y_i$，每个 i 对应于一个数据点，希望建立一个模型\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w x_i + b\n",
    "$$\n",
    "\n",
    "$\\hat{y}_i$ 是我们预测的结果，希望通过 $\\hat{y}_i$ 来拟合目标 $y_i$，通俗来讲就是找到这个函数拟合 $y_i$ 使得误差最小，即最小化\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n(\\hat{y}_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如何最小化这个误差呢？\n",
    "\n",
    "这里需要用到**梯度下降**，这是我们接触到的第一个优化算法，非常简单，但是却非常强大，在深度学习中被大量使用，所以让我们从简单的例子出发了解梯度下降法的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "在梯度下降法中，我们首先要明确梯度的概念，随后我们再了解如何使用梯度进行下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "梯度在数学上就是导数，如果是一个多元函数，那么梯度就是偏导数。比如一个函数f(x, y)，那么 f 的梯度就是 \n",
    "\n",
    "$$\n",
    "(\\frac{\\partial f}{\\partial x},\\ \\frac{\\partial f}{\\partial y})\n",
    "$$\n",
    "\n",
    "可以称为 grad f(x, y) 或者 $\\nabla f(x, y)$。具体某一点 $(x_0,\\ y_0)$ 的梯度就是 $\\nabla f(x_0,\\ y_0)$。\n",
    "\n",
    "下面这个图片是 $f(x) = x^2$ 这个函数在 x=1 处的梯度\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarbuh2j3j30ba0b80sy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度有什么意义呢？从几何意义来讲，一个点的梯度值是这个函数变化最快的地方，具体来说，对于函数 f(x, y)，在点 $(x_0, y_0)$ 处，沿着梯度 $\\nabla f(x_0,\\ y_0)$ 的方向，函数增加最快，也就是说沿着梯度的方向，我们能够更快地找到函数的极大值点，或者反过来沿着梯度的反方向，我们能够更快地找到函数的最小值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法\n",
    "有了对梯度的理解，我们就能了解梯度下降发的原理了。上面我们需要最小化这个误差，也就是需要找到这个误差的最小值点，那么沿着梯度的反方向我们就能够找到这个最小值点。\n",
    "\n",
    "我们可以来看一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。\n",
    "\n",
    "类比我们的问题，就是沿着梯度的反方向，我们不断改变 w 和 b 的值，最终找到一组最好的 w 和 b 使得误差最小。\n",
    "\n",
    "在更新的时候，我们需要决定每次更新的幅度，比如在下山的例子中，我们需要每次往下走的那一步的长度，这个长度称为学习率，用 $\\eta$ 表示，这个学习率非常重要，不同的学习率都会导致不同的结果，学习率太小会导致下降非常缓慢，学习率太大又会导致跳动非常明显，可以看看下面的例子\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79ly1fmgn23lnzjg30980gogso.gif)\n",
    "\n",
    "可以看到上面的学习率较为合适，而下面的学习率太大，就会导致不断跳动\n",
    "\n",
    "最后我们的更新公式就是\n",
    "\n",
    "$$\n",
    "w := w - \\eta \\frac{\\partial f(w,\\ b)}{\\partial w} \\\\\n",
    "b := b - \\eta \\frac{\\partial f(w,\\ b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "通过不断地迭代更新，最终我们能够找到一组最优的 w 和 b，这就是梯度下降法的原理。\n",
    "\n",
    "最后可以通过这张图形象地说明一下这个方法\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarxsltfqj30gx091gn4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是原理部分，下面通过一个例子来进一步学习线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dd373b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据 x 和 y\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042],\n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],\n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827],\n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1136537f0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出图像\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换成 Tensor\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# 定义参数 w 和 b\n",
    "w = Variable(torch.randn(1), requires_grad=True) # 随机初始化\n",
    "b = Variable(torch.zeros(1), requires_grad=True) # 使用 0 进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3991]) tensor([ 0.])\n",
      "tensor([[  3.3000],\n",
      "        [  4.4000],\n",
      "        [  5.5000],\n",
      "        [  6.7100],\n",
      "        [  6.9300],\n",
      "        [  4.1680],\n",
      "        [  9.7790],\n",
      "        [  6.1820],\n",
      "        [  7.5900],\n",
      "        [  2.1670],\n",
      "        [  7.0420],\n",
      "        [ 10.7910],\n",
      "        [  5.3130],\n",
      "        [  7.9970],\n",
      "        [  3.1000]]) tensor([[ 1.7000],\n",
      "        [ 2.7600],\n",
      "        [ 2.0900],\n",
      "        [ 3.1900],\n",
      "        [ 1.6940],\n",
      "        [ 1.5730],\n",
      "        [ 3.3660],\n",
      "        [ 2.5960],\n",
      "        [ 2.5300],\n",
      "        [ 1.2210],\n",
      "        [ 2.8270],\n",
      "        [ 3.4650],\n",
      "        [ 1.6500],\n",
      "        [ 2.9040],\n",
      "        [ 1.3000]])\n",
      "Help on Tensor in module torch object:\n",
      "\n",
      "class Tensor(torch._C._TensorBase)\n",
      " |  Method resolution order:\n",
      " |      Tensor\n",
      " |      torch._C._TensorBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(...)\n",
      " |      abs() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.abs`\n",
      " |  \n",
      " |  __array__(self, dtype=None)\n",
      " |      # Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\n",
      " |  \n",
      " |  __array_wrap__(self, array)\n",
      " |      # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
      " |      # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __eq__ = eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.eq`\n",
      " |  \n",
      " |  __format__(self, format_spec)\n",
      " |      default object formatter\n",
      " |  \n",
      " |  __ge__ = ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ge`\n",
      " |  \n",
      " |  __gt__ = gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gt`\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __itruediv__ = __idiv__(...)\n",
      " |  \n",
      " |  __le__ = le(...)\n",
      " |      le(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.le`\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__ = lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lt`\n",
      " |  \n",
      " |  __ne__ = ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ne`\n",
      " |  \n",
      " |  __neg__ = neg(...)\n",
      " |      neg() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.neg`\n",
      " |  \n",
      " |  __pow__ = pow(...)\n",
      " |      pow(exponent) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.pow`\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __reduce_ex__(self, proto)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  argmax(self, dim=None, keepdim=False)\n",
      " |      See :func:`torch.argmax`\n",
      " |  \n",
      " |  argmin(self, dim=None, keepdim=False)\n",
      " |      See :func:`torch.argmin`\n",
      " |  \n",
      " |  backward(self, gradient=None, retain_graph=None, create_graph=False)\n",
      " |      Computes the gradient of current tensor w.r.t. graph leaves.\n",
      " |      \n",
      " |      The graph is differentiated using the chain rule. If the tensor is\n",
      " |      non-scalar (i.e. its data has more than one element) and requires\n",
      " |      gradient, the function additionally requires specifying ``gradient``.\n",
      " |      It should be a tensor of matching type and location, that contains\n",
      " |      the gradient of the differentiated function w.r.t. ``self``.\n",
      " |      \n",
      " |      This function accumulates gradients in the leaves - you might need to\n",
      " |      zero them before calling it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          gradient (Tensor or None): Gradient w.r.t. the\n",
      " |              tensor. If it is a tensor, it will be automatically converted\n",
      " |              to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      " |              None values can be specified for scalar Tensors or ones that\n",
      " |              don't require grad. If a None value would be acceptable then\n",
      " |              this argument is optional.\n",
      " |          retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      " |              the grads will be freed. Note that in nearly all cases setting\n",
      " |              this option to True is not needed and often can be worked around\n",
      " |              in a much more efficient way. Defaults to the value of\n",
      " |              ``create_graph``.\n",
      " |          create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      " |              be constructed, allowing to compute higher order derivative\n",
      " |              products. Defaults to ``False``.\n",
      " |  \n",
      " |  btrifact(self, info=None, pivot=True)\n",
      " |      See :func:`torch.btrifact`\n",
      " |  \n",
      " |  detach(...)\n",
      " |      Returns a new Tensor, detached from the current graph.\n",
      " |      \n",
      " |      The result will never require gradient.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |        Returned Tensor uses the same data tensor as the original one.\n",
      " |        In-place modifications on either of them will be seen, and may trigger\n",
      " |        errors in correctness checks.\n",
      " |  \n",
      " |  detach_(...)\n",
      " |      Detaches the Tensor from the graph that created it, making it a leaf.\n",
      " |      Views cannot be detached in-place.\n",
      " |  \n",
      " |  expand_as(self, tensor)\n",
      " |  \n",
      " |  index_add(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_copy(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_fill(self, dim, index, value)\n",
      " |  \n",
      " |  is_pinned(self)\n",
      " |      Returns true if this tensor resides in pinned memory\n",
      " |  \n",
      " |  is_shared(self)\n",
      " |      Checks if tensor is in shared memory.\n",
      " |      \n",
      " |      This is always ``True`` for CUDA tensors.\n",
      " |  \n",
      " |  masked_copy(self, mask, tensor)\n",
      " |  \n",
      " |  masked_copy_(self, mask, tensor)\n",
      " |  \n",
      " |  masked_fill(self, mask, value)\n",
      " |  \n",
      " |  masked_scatter(self, mask, tensor)\n",
      " |  \n",
      " |  register_hook(self, hook)\n",
      " |      Registers a backward hook.\n",
      " |      \n",
      " |      The hook will be called every time a gradient with respect to the\n",
      " |      Tensor is computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(grad) -> Tensor or None\n",
      " |      \n",
      " |      The hook should not modify its argument, but it can optionally return\n",
      " |      a new gradient which will be used in place of :attr:`grad`.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v.grad\n",
      " |      \n",
      " |           2\n",
      " |           4\n",
      " |           6\n",
      " |          [torch.FloatTensor of size (3,)]\n",
      " |      \n",
      " |          >>> h.remove()  # removes the hook\n",
      " |  \n",
      " |  reinforce(self, reward)\n",
      " |  \n",
      " |  resize(self, *sizes)\n",
      " |  \n",
      " |  resize_as(self, tensor)\n",
      " |  \n",
      " |  retain_grad(self)\n",
      " |      Enables .grad attribute for non-leaf Tensors.\n",
      " |  \n",
      " |  scatter(self, dim, index, source)\n",
      " |  \n",
      " |  scatter_add(self, dim, index, source)\n",
      " |  \n",
      " |  share_memory_(self)\n",
      " |      Moves the underlying storage to shared memory.\n",
      " |      \n",
      " |      This is a no-op if the underlying storage is already in shared memory\n",
      " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      " |  \n",
      " |  split(self, split_size, dim=0)\n",
      " |      See :func:`torch.split`\n",
      " |  \n",
      " |  unique(self, sorted=False, return_inverse=False)\n",
      " |      Returns the unique scalar elements of the tensor as a 1-D tensor.\n",
      " |      \n",
      " |      See :func:`torch.unique`\n",
      " |  \n",
      " |  view_as(self, tensor)\n",
      " |      view_as(other) -> Tensor\n",
      " |      \n",
      " |      View this tensor as the same size as :attr:`other`.\n",
      " |      ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other.size()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C._TensorBase:\n",
      " |  \n",
      " |  __add__(...)\n",
      " |  \n",
      " |  __and__(...)\n",
      " |  \n",
      " |  __bool__(...)\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __div__(...)\n",
      " |  \n",
      " |  __float__(...)\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __iadd__(...)\n",
      " |  \n",
      " |  __iand__(...)\n",
      " |  \n",
      " |  __idiv__(...)\n",
      " |  \n",
      " |  __ilshift__(...)\n",
      " |  \n",
      " |  __imul__(...)\n",
      " |  \n",
      " |  __index__(...)\n",
      " |  \n",
      " |  __int__(...)\n",
      " |  \n",
      " |  __invert__(...)\n",
      " |  \n",
      " |  __ior__(...)\n",
      " |  \n",
      " |  __irshift__(...)\n",
      " |  \n",
      " |  __isub__(...)\n",
      " |  \n",
      " |  __ixor__(...)\n",
      " |  \n",
      " |  __long__(...)\n",
      " |  \n",
      " |  __lshift__(...)\n",
      " |  \n",
      " |  __matmul__(...)\n",
      " |  \n",
      " |  __mod__(...)\n",
      " |  \n",
      " |  __mul__(...)\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __nonzero__(...)\n",
      " |  \n",
      " |  __or__(...)\n",
      " |  \n",
      " |  __radd__(...)\n",
      " |  \n",
      " |  __rmul__(...)\n",
      " |  \n",
      " |  __rshift__(...)\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sub__(...)\n",
      " |  \n",
      " |  __truediv__(...)\n",
      " |  \n",
      " |  __xor__(...)\n",
      " |  \n",
      " |  abs(...)\n",
      " |      abs() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.abs`\n",
      " |  \n",
      " |  abs_(...)\n",
      " |      abs_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.abs`\n",
      " |  \n",
      " |  acos(...)\n",
      " |      acos() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.acos`\n",
      " |  \n",
      " |  acos_(...)\n",
      " |      acos_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.acos`\n",
      " |  \n",
      " |  add(...)\n",
      " |      add(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.add`\n",
      " |  \n",
      " |  add_(...)\n",
      " |      add_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.add`\n",
      " |  \n",
      " |  addbmm(...)\n",
      " |      addbmm(beta=1, mat, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addbmm`\n",
      " |  \n",
      " |  addbmm_(...)\n",
      " |      addbmm_(beta=1, mat, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addbmm`\n",
      " |  \n",
      " |  addcdiv(...)\n",
      " |      addcdiv(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcdiv`\n",
      " |  \n",
      " |  addcdiv_(...)\n",
      " |      addcdiv_(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcdiv`\n",
      " |  \n",
      " |  addcmul(...)\n",
      " |      addcmul(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcmul`\n",
      " |  \n",
      " |  addcmul_(...)\n",
      " |      addcmul_(value=1, tensor1, tensor2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcmul`\n",
      " |  \n",
      " |  addmm(...)\n",
      " |      addmm(beta=1, mat, alpha=1, mat1, mat2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmm`\n",
      " |  \n",
      " |  addmm_(...)\n",
      " |      addmm_(beta=1, mat, alpha=1, mat1, mat2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmm`\n",
      " |  \n",
      " |  addmv(...)\n",
      " |      addmv(beta=1, tensor, alpha=1, mat, vec) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmv`\n",
      " |  \n",
      " |  addmv_(...)\n",
      " |      addmv_(beta=1, tensor, alpha=1, mat, vec) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmv`\n",
      " |  \n",
      " |  addr(...)\n",
      " |      addr(beta=1, alpha=1, vec1, vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addr`\n",
      " |  \n",
      " |  addr_(...)\n",
      " |      addr_(beta=1, alpha=1, vec1, vec2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addr`\n",
      " |  \n",
      " |  all(...)\n",
      " |      all() -> bool\n",
      " |      \n",
      " |      Returns ``True`` if all elements in the tensor are non-zero, ``False`` otherwise.\n",
      " |  \n",
      " |  allclose(...)\n",
      " |  \n",
      " |  any(...)\n",
      " |      any() -> bool\n",
      " |      \n",
      " |      Returns ``True`` if any elements in the tensor are non-zero, ``False`` otherwise.\n",
      " |  \n",
      " |  apply_(...)\n",
      " |      apply_(callable) -> Tensor\n",
      " |      \n",
      " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
      " |      each element with the value returned by :attr:`callable`.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This function only works with CPU tensors and should not be used in code\n",
      " |          sections that require high performance.\n",
      " |  \n",
      " |  as_strided(...)\n",
      " |  \n",
      " |  as_strided_(...)\n",
      " |  \n",
      " |  asin(...)\n",
      " |      asin() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.asin`\n",
      " |  \n",
      " |  asin_(...)\n",
      " |      asin_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.asin`\n",
      " |  \n",
      " |  atan(...)\n",
      " |      atan() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.atan`\n",
      " |  \n",
      " |  atan2(...)\n",
      " |      atan2(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.atan2`\n",
      " |  \n",
      " |  atan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.atan2`\n",
      " |  \n",
      " |  atan_(...)\n",
      " |      atan_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.atan`\n",
      " |  \n",
      " |  baddbmm(...)\n",
      " |      baddbmm(beta=1, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.baddbmm`\n",
      " |  \n",
      " |  baddbmm_(...)\n",
      " |      baddbmm_(beta=1, alpha=1, batch1, batch2) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.baddbmm`\n",
      " |  \n",
      " |  bernoulli(...)\n",
      " |      bernoulli() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bernoulli`\n",
      " |  \n",
      " |  bernoulli_(...)\n",
      " |      bernoulli_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bernoulli`\n",
      " |  \n",
      " |  bmm(...)\n",
      " |      bmm(batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bmm`\n",
      " |  \n",
      " |  btrifact_with_info(...)\n",
      " |      btrifact_with_info(pivot=True) -> (Tensor, Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.btrifact_with_info`\n",
      " |  \n",
      " |  btrisolve(...)\n",
      " |  \n",
      " |  byte(...)\n",
      " |      byte() -> Tensor\n",
      " |      \n",
      " |      ``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.\n",
      " |  \n",
      " |  cauchy_(...)\n",
      " |      cauchy_(median=0, sigma=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills the tensor with numbers drawn from the Cauchy distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - median)^2 + \\sigma^2}\n",
      " |  \n",
      " |  ceil(...)\n",
      " |      ceil() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ceil`\n",
      " |  \n",
      " |  ceil_(...)\n",
      " |      ceil_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ceil`\n",
      " |  \n",
      " |  char(...)\n",
      " |      char() -> Tensor\n",
      " |      \n",
      " |      ``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.\n",
      " |  \n",
      " |  chunk(...)\n",
      " |      chunk(chunks, dim=0) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.chunk`\n",
      " |  \n",
      " |  clamp(...)\n",
      " |      clamp(min, max) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.clamp`\n",
      " |  \n",
      " |  clamp_(...)\n",
      " |      clamp_(min, max) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.clamp`\n",
      " |  \n",
      " |  clone(...)\n",
      " |      clone() -> Tensor\n",
      " |      \n",
      " |      Returns a copy of the :attr:`self` tensor. The copy has the same size and data\n",
      " |      type as :attr:`self`.\n",
      " |  \n",
      " |  coalesce(...)\n",
      " |  \n",
      " |  contiguous(...)\n",
      " |      contiguous() -> Tensor\n",
      " |      \n",
      " |      Returns a contiguous tensor containing the same data as :attr:`self` tensor. If\n",
      " |      :attr:`self` tensor is contiguous, this function returns the :attr:`self`\n",
      " |      tensor.\n",
      " |  \n",
      " |  conv_tbc(...)\n",
      " |  \n",
      " |  copy_(...)\n",
      " |      copy_(src, non_blocking=False) -> Tensor\n",
      " |      \n",
      " |      Copies the elements from :attr:`src` into :attr:`self` tensor and returns\n",
      " |      :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`src` tensor must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the :attr:`self` tensor. It may be of a different data type or reside on a\n",
      " |      different device.\n",
      " |      \n",
      " |      Args:\n",
      " |          src (Tensor): the source tensor to copy from\n",
      " |          non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n",
      " |              the copy may occur asynchronously with respect to the host. For other\n",
      " |              cases, this argument has no effect.\n",
      " |  \n",
      " |  cos(...)\n",
      " |      cos() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cos`\n",
      " |  \n",
      " |  cos_(...)\n",
      " |      cos_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cos`\n",
      " |  \n",
      " |  cosh(...)\n",
      " |      cosh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cosh`\n",
      " |  \n",
      " |  cosh_(...)\n",
      " |      cosh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cosh`\n",
      " |  \n",
      " |  cpu(...)\n",
      " |  \n",
      " |  cross(...)\n",
      " |      cross(other, dim=-1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cross`\n",
      " |  \n",
      " |  cuda(...)\n",
      " |      cuda(device=None, non_blocking=False) -> Tensor\n",
      " |      \n",
      " |      Returns a copy of this object in CUDA memory.\n",
      " |      \n",
      " |      If this object is already in CUDA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination GPU device.\n",
      " |              Defaults to the current CUDA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |  \n",
      " |  cumprod(...)\n",
      " |      cumprod(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumprod`\n",
      " |  \n",
      " |  cumsum(...)\n",
      " |      cumsum(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumsum`\n",
      " |  \n",
      " |  data_ptr(...)\n",
      " |      data_ptr() -> int\n",
      " |      \n",
      " |      Returns the address of the first element of :attr:`self` tensor.\n",
      " |  \n",
      " |  det(...)\n",
      " |      det() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.det`\n",
      " |  \n",
      " |  diag(...)\n",
      " |      diag(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diag`\n",
      " |  \n",
      " |  digamma(...)\n",
      " |  \n",
      " |  digamma_(...)\n",
      " |  \n",
      " |  dim(...)\n",
      " |      dim() -> int\n",
      " |      \n",
      " |      Returns the number of dimensions of :attr:`self` tensor.\n",
      " |  \n",
      " |  dist(...)\n",
      " |      dist(other, p=2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.dist`\n",
      " |  \n",
      " |  div(...)\n",
      " |      div(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.div`\n",
      " |  \n",
      " |  div_(...)\n",
      " |      div_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.div`\n",
      " |  \n",
      " |  dot(...)\n",
      " |      dot(tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.dot`\n",
      " |  \n",
      " |  double(...)\n",
      " |      double() -> Tensor\n",
      " |      \n",
      " |      ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.\n",
      " |  \n",
      " |  eig(...)\n",
      " |      eig(eigenvectors=False) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.eig`\n",
      " |  \n",
      " |  element_size(...)\n",
      " |      element_size() -> int\n",
      " |      \n",
      " |      Returns the size in bytes of an individual element.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> torch.tensor([]).element_size()\n",
      " |          4\n",
      " |          >>> torch.tensor([], dtype=torch.uint8).element_size()\n",
      " |          1\n",
      " |  \n",
      " |  eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.eq`\n",
      " |  \n",
      " |  eq_(...)\n",
      " |      eq_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.eq`\n",
      " |  \n",
      " |  equal(...)\n",
      " |      equal(other) -> bool\n",
      " |      \n",
      " |      See :func:`torch.equal`\n",
      " |  \n",
      " |  erf(...)\n",
      " |      erf() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.erf`\n",
      " |  \n",
      " |  erf_(...)\n",
      " |  \n",
      " |  erfinv(...)\n",
      " |      erfinv() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.erfinv`\n",
      " |  \n",
      " |  erfinv_(...)\n",
      " |  \n",
      " |  exp(...)\n",
      " |      exp() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.exp`\n",
      " |  \n",
      " |  exp_(...)\n",
      " |      exp_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.exp`\n",
      " |  \n",
      " |  expand(...)\n",
      " |      expand(*sizes) -> Tensor\n",
      " |      \n",
      " |      Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
      " |      to a larger size.\n",
      " |      \n",
      " |      Passing -1 as the size for a dimension means not changing the size of\n",
      " |      that dimension.\n",
      " |      \n",
      " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
      " |      new ones will be appended at the front. For the new dimensions, the\n",
      " |      size cannot be set to -1.\n",
      " |      \n",
      " |      Expanding a tensor does not allocate new memory, but only creates a\n",
      " |      new view on the existing tensor where a dimension of size one is\n",
      " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
      " |      memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): the desired expanded size\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1], [2], [3]])\n",
      " |          >>> x.size()\n",
      " |          torch.Size([3, 1])\n",
      " |          >>> x.expand(3, 4)\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |          >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |  \n",
      " |  expm1(...)\n",
      " |      expm1() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.expm1`\n",
      " |  \n",
      " |  expm1_(...)\n",
      " |      expm1_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.expm1`\n",
      " |  \n",
      " |  exponential_(...)\n",
      " |      exponential_(lambd=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements drawn from the exponential distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\lambda e^{-\\lambda x}\n",
      " |  \n",
      " |  fft(...)\n",
      " |      fft(signal_ndim, normalized=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fft`\n",
      " |  \n",
      " |  fill_(...)\n",
      " |      fill_(value) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with the specified value.\n",
      " |  \n",
      " |  float(...)\n",
      " |      float() -> Tensor\n",
      " |      \n",
      " |      ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n",
      " |  \n",
      " |  floor(...)\n",
      " |      floor() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.floor`\n",
      " |  \n",
      " |  floor_(...)\n",
      " |      floor_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.floor`\n",
      " |  \n",
      " |  fmod(...)\n",
      " |      fmod(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fmod`\n",
      " |  \n",
      " |  fmod_(...)\n",
      " |      fmod_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.fmod`\n",
      " |  \n",
      " |  frac(...)\n",
      " |      frac() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.frac`\n",
      " |  \n",
      " |  frac_(...)\n",
      " |      frac_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.frac`\n",
      " |  \n",
      " |  gather(...)\n",
      " |      gather(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gather`\n",
      " |  \n",
      " |  ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ge`\n",
      " |  \n",
      " |  ge_(...)\n",
      " |      ge_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ge`\n",
      " |  \n",
      " |  gels(...)\n",
      " |      gels(A) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gels`\n",
      " |  \n",
      " |  geometric_(...)\n",
      " |      geometric_(p, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements drawn from the geometric distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(X=k) = (1 - p)^{k - 1} p\n",
      " |  \n",
      " |  geqrf(...)\n",
      " |      geqrf() -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.geqrf`\n",
      " |  \n",
      " |  ger(...)\n",
      " |      ger(vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ger`\n",
      " |  \n",
      " |  gesv(...)\n",
      " |      gesv(A) -> Tensor, Tensor\n",
      " |      \n",
      " |      See :func:`torch.gesv`\n",
      " |  \n",
      " |  get_device(...)\n",
      " |  \n",
      " |  gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gt`\n",
      " |  \n",
      " |  gt_(...)\n",
      " |      gt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.gt`\n",
      " |  \n",
      " |  half(...)\n",
      " |      half() -> Tensor\n",
      " |      \n",
      " |      ``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.\n",
      " |  \n",
      " |  histc(...)\n",
      " |      histc(bins=100, min=0, max=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.histc`\n",
      " |  \n",
      " |  ifft(...)\n",
      " |      ifft(signal_ndim, normalized=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ifft`\n",
      " |  \n",
      " |  index(...)\n",
      " |      index(m) -> Tensor\n",
      " |      \n",
      " |      Selects elements from :attr:`self` tensor using a binary mask or along a given\n",
      " |      dimension. The expression ``tensor.index(m)`` is equivalent to ``tensor[m]``.\n",
      " |      \n",
      " |      Args:\n",
      " |          m (int or ByteTensor or slice): the dimension or mask used to select elements\n",
      " |  \n",
      " |  index_add_(...)\n",
      " |      index_add_(dim, index, tensor) -> Tensor\n",
      " |      \n",
      " |      Accumulate the elements of :attr:`tensor` into the :attr:`self` tensor by adding\n",
      " |      to the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
      " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is added to the\n",
      " |      ``j``\\ th row of :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
      " |          tensor (Tensor): the tensor containing values to add\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.ones(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_add_(0, index, t)\n",
      " |          tensor([[  2.,   3.,   4.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  8.,   9.,  10.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  5.,   6.,   7.]])\n",
      " |  \n",
      " |  index_copy_(...)\n",
      " |      index_copy_(dim, index, tensor) -> Tensor\n",
      " |      \n",
      " |      Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting\n",
      " |      the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
      " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is copied to the\n",
      " |      ``j``\\ th row of :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
      " |          tensor (Tensor): the tensor containing values to copy\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.zeros(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_copy_(0, index, t)\n",
      " |          tensor([[ 1.,  2.,  3.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |  \n",
      " |  index_fill_(...)\n",
      " |      index_fill_(dim, index, val) -> Tensor\n",
      " |      \n",
      " |      Fills the elements of the :attr:`self` tensor with value :attr:`val` by\n",
      " |      selecting the indices in the order given in :attr:`index`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`self` tensor to fill in\n",
      " |          val (float): the value to fill with\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 2])\n",
      " |          >>> x.index_fill_(1, index, -1)\n",
      " |          tensor([[-1.,  2., -1.],\n",
      " |                  [-1.,  5., -1.],\n",
      " |                  [-1.,  8., -1.]])\n",
      " |  \n",
      " |  index_put_(...)\n",
      " |      index_put_(indices, value) -> Tensor\n",
      " |      \n",
      " |      Puts values from the tensor :attr:`value` into the tensor :attr:`self` using\n",
      " |      the indices specified in :attr:`indices` (which is a tuple of Tensors). The\n",
      " |      expression ``tensor.index_put_(indices, value)`` is equivalent to\n",
      " |      ``tensor[indices] = value``. Returns :attr:`self`.\n",
      " |      \n",
      " |      Args:\n",
      " |          indices (tuple of LongTensor): tensors used to index into `self`.\n",
      " |          value (Tensor): tensor of same dtype as `self`.\n",
      " |  \n",
      " |  index_select(...)\n",
      " |      index_select(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.index_select`\n",
      " |  \n",
      " |  int(...)\n",
      " |      int() -> Tensor\n",
      " |      \n",
      " |      ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.\n",
      " |  \n",
      " |  inverse(...)\n",
      " |      inverse() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.inverse`\n",
      " |  \n",
      " |  irfft(...)\n",
      " |      irfft(signal_ndim, normalized=False, onesided=True, signal_sizes=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.irfft`\n",
      " |  \n",
      " |  is_coalesced(...)\n",
      " |  \n",
      " |  is_contiguous(...)\n",
      " |      is_contiguous() -> bool\n",
      " |      \n",
      " |      Returns True if :attr:`self` tensor is contiguous in memory in C order.\n",
      " |  \n",
      " |  is_distributed(...)\n",
      " |  \n",
      " |  is_floating_point(...)\n",
      " |  \n",
      " |  is_nonzero(...)\n",
      " |  \n",
      " |  is_same_size(...)\n",
      " |  \n",
      " |  is_set_to(...)\n",
      " |      is_set_to(tensor) -> bool\n",
      " |      \n",
      " |      Returns True if this object refers to the same ``THTensor`` object from the\n",
      " |      Torch C API as the given tensor.\n",
      " |  \n",
      " |  is_signed(...)\n",
      " |  \n",
      " |  isclose(...)\n",
      " |  \n",
      " |  item(...)\n",
      " |      item() -> number\n",
      " |      \n",
      " |      Returns the value of this tensor as a standard Python number. This only works\n",
      " |      for tensors with one element.\n",
      " |      \n",
      " |      This operation is not differentiable.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1.0])\n",
      " |          >>> x.item()\n",
      " |          1.0\n",
      " |  \n",
      " |  kthvalue(...)\n",
      " |      kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.kthvalue`\n",
      " |  \n",
      " |  le(...)\n",
      " |      le(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.le`\n",
      " |  \n",
      " |  le_(...)\n",
      " |      le_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.le`\n",
      " |  \n",
      " |  lerp(...)\n",
      " |      lerp(start, end, weight) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lerp`\n",
      " |  \n",
      " |  lerp_(...)\n",
      " |      lerp_(start, end, weight) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lerp`\n",
      " |  \n",
      " |  lgamma(...)\n",
      " |  \n",
      " |  lgamma_(...)\n",
      " |  \n",
      " |  log(...)\n",
      " |      log() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log`\n",
      " |  \n",
      " |  log10(...)\n",
      " |      log10() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log10`\n",
      " |  \n",
      " |  log10_(...)\n",
      " |      log10_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log10`\n",
      " |  \n",
      " |  log1p(...)\n",
      " |      log1p() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log1p`\n",
      " |  \n",
      " |  log1p_(...)\n",
      " |      log1p_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log1p`\n",
      " |  \n",
      " |  log2(...)\n",
      " |      log2() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log2`\n",
      " |  \n",
      " |  log2_(...)\n",
      " |      log2_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log2`\n",
      " |  \n",
      " |  log_(...)\n",
      " |      log_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log`\n",
      " |  \n",
      " |  log_normal_(...)\n",
      " |      log_normal_(mean=1, std=2, *, generator=None)\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers samples from the log-normal distribution\n",
      " |      parameterized by the given mean (µ) and standard deviation (σ).\n",
      " |      Note that :attr:`mean` and :attr:`stdv` are the mean and standard deviation of\n",
      " |      the underlying normal distribution, and not of the returned distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\dfrac{(\\ln x - \\mu)^2}{2\\sigma^2}}\n",
      " |  \n",
      " |  logdet(...)\n",
      " |      logdet() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logdet`\n",
      " |  \n",
      " |  long(...)\n",
      " |      long() -> Tensor\n",
      " |      \n",
      " |      ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n",
      " |  \n",
      " |  lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lt`\n",
      " |  \n",
      " |  lt_(...)\n",
      " |      lt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lt`\n",
      " |  \n",
      " |  map2_(...)\n",
      " |  \n",
      " |  map_(...)\n",
      " |      map_(tensor, callable)\n",
      " |      \n",
      " |      Applies :attr:`callable` for each element in :attr:`self` tensor and the given\n",
      " |      :attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and\n",
      " |      the given :attr:`tensor` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      " |      \n",
      " |      The :attr:`callable` should have the signature::\n",
      " |      \n",
      " |          def callable(a, b) -> number\n",
      " |  \n",
      " |  masked_fill_(...)\n",
      " |      masked_fill_(mask, value)\n",
      " |      \n",
      " |      Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
      " |      one. The shape of :attr:`mask` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (ByteTensor): the binary mask\n",
      " |          value (float): the value to fill in with\n",
      " |  \n",
      " |  masked_scatter_(...)\n",
      " |      masked_scatter_(mask, source)\n",
      " |      \n",
      " |      Copies elements from :attr:`source` into :attr:`self` tensor at positions where\n",
      " |      the :attr:`mask` is one.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
      " |      as many elements as the number of ones in :attr:`mask`\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (ByteTensor): the binary mask\n",
      " |          source (Tensor): the tensor to copy from\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
      " |          :attr:`source` tensor.\n",
      " |  \n",
      " |  masked_select(...)\n",
      " |      masked_select(mask) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.masked_select`\n",
      " |  \n",
      " |  matmul(...)\n",
      " |      matmul(tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.matmul`\n",
      " |  \n",
      " |  max(...)\n",
      " |      max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.max`\n",
      " |  \n",
      " |  mean(...)\n",
      " |      mean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.mean`\n",
      " |  \n",
      " |  median(...)\n",
      " |      median(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.median`\n",
      " |  \n",
      " |  min(...)\n",
      " |      min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.min`\n",
      " |  \n",
      " |  mm(...)\n",
      " |      mm(mat2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mm`\n",
      " |  \n",
      " |  mode(...)\n",
      " |      mode(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.mode`\n",
      " |  \n",
      " |  mul(...)\n",
      " |      mul(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mul`\n",
      " |  \n",
      " |  mul_(...)\n",
      " |      mul_(value)\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.mul`\n",
      " |  \n",
      " |  multinomial(...)\n",
      " |      multinomial(num_samples, replacement=False, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.multinomial`\n",
      " |  \n",
      " |  mv(...)\n",
      " |      mv(vec) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mv`\n",
      " |  \n",
      " |  narrow(...)\n",
      " |      narrow(dimension, start, length) -> Tensor\n",
      " |      \n",
      " |      Returns a new tensor that is a narrowed version of :attr:`self` tensor. The\n",
      " |      dimension :attr:`dim` is narrowed from :attr:`start` to :attr:`start + length`. The\n",
      " |      returned tensor and :attr:`self` tensor share the same underlying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          dimension (int): the dimension along which to narrow\n",
      " |          start (int): the starting dimension\n",
      " |          length (int): the distance to the ending dimension\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |          >>> x.narrow(0, 0, 2)\n",
      " |          tensor([[ 1,  2,  3],\n",
      " |                  [ 4,  5,  6]])\n",
      " |          >>> x.narrow(1, 1, 2)\n",
      " |          tensor([[ 2,  3],\n",
      " |                  [ 5,  6],\n",
      " |                  [ 8,  9]])\n",
      " |  \n",
      " |  ndimension(...)\n",
      " |      ndimension() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |  \n",
      " |  ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ne`\n",
      " |  \n",
      " |  ne_(...)\n",
      " |      ne_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ne`\n",
      " |  \n",
      " |  neg(...)\n",
      " |      neg() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.neg`\n",
      " |  \n",
      " |  neg_(...)\n",
      " |      neg_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.neg`\n",
      " |  \n",
      " |  nelement(...)\n",
      " |      nelement() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.numel`\n",
      " |  \n",
      " |  new(...)\n",
      " |  \n",
      " |  new_empty(...)\n",
      " |      new_empty(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with uninitialized data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty((2, 3))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |  \n",
      " |  new_full(...)\n",
      " |      new_full(size, fill_value, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          fill_value (scalar): the number to fill the output tensor with.\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.float64)\n",
      " |          >>> tensor.new_full((3, 4), 3.141592)\n",
      " |          tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n",
      " |  \n",
      " |  new_ones(...)\n",
      " |      new_ones(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with ``1``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.tensor((), dtype=torch.int32)\n",
      " |          >>> tensor.new_ones((2, 3))\n",
      " |          tensor([[ 1,  1,  1],\n",
      " |                  [ 1,  1,  1]], dtype=torch.int32)\n",
      " |  \n",
      " |  new_tensor(...)\n",
      " |      new_tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      " |      \n",
      " |      Returns a new Tensor with :attr:`data` as the tensor data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n",
      " |          ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      " |          or :func:`torch.Tensor.detach`.\n",
      " |          If you have a numpy array and want to avoid a copy, use\n",
      " |          :func:`torch.from_numpy`.\n",
      " |      \n",
      " |      Args:\n",
      " |          data (array_like): The returned Tensor copies :attr:`data`.\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.int8)\n",
      " |          >>> data = [[0, 1], [2, 3]]\n",
      " |          >>> tensor.new_tensor(data)\n",
      " |          tensor([[ 0,  1],\n",
      " |                  [ 2,  3]], dtype=torch.int8)\n",
      " |  \n",
      " |  new_zeros(...)\n",
      " |      new_zeros(size, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with ``0``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.tensor((), dtype=torch.float64)\n",
      " |          >>> tensor.new_ones((2, 3))\n",
      " |          tensor([[ 1.,  1.,  1.],\n",
      " |                  [ 1.,  1.,  1.]], dtype=torch.float64)\n",
      " |  \n",
      " |  nonzero(...)\n",
      " |      nonzero() -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.nonzero`\n",
      " |  \n",
      " |  norm(...)\n",
      " |      norm(p=2, dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.norm`\n",
      " |  \n",
      " |  normal_(...)\n",
      " |      normal_(mean=0, std=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements samples from the normal distribution\n",
      " |      parameterized by :attr:`mean` and :attr:`std`.\n",
      " |  \n",
      " |  numel(...)\n",
      " |      numel() -> int\n",
      " |      \n",
      " |      See :func:`torch.numel`\n",
      " |  \n",
      " |  numpy(...)\n",
      " |      numpy() -> numpy.ndarray\n",
      " |      \n",
      " |      Returns :attr:`self` tensor as a NumPy :class:`ndarray`. This tensor and the\n",
      " |      returned :class:`ndarray` share the same underlying storage. Changes to\n",
      " |      :attr:`self` tensor will be reflected in the :class:`ndarray` and vice versa.\n",
      " |  \n",
      " |  orgqr(...)\n",
      " |      orgqr(input2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.orgqr`\n",
      " |  \n",
      " |  ormqr(...)\n",
      " |      ormqr(input2, input3, left=True, transpose=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ormqr`\n",
      " |  \n",
      " |  permute(...)\n",
      " |  \n",
      " |  pin_memory(...)\n",
      " |  \n",
      " |  polygamma(...)\n",
      " |  \n",
      " |  polygamma_(...)\n",
      " |  \n",
      " |  potrf(...)\n",
      " |      potrf(upper=True) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.potrf`\n",
      " |  \n",
      " |  potri(...)\n",
      " |      potri(upper=True) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.potri`\n",
      " |  \n",
      " |  potrs(...)\n",
      " |      potrs(input2, upper=True) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.potrs`\n",
      " |  \n",
      " |  pow(...)\n",
      " |      pow(exponent) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.pow`\n",
      " |  \n",
      " |  pow_(...)\n",
      " |      pow_(exponent) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.pow`\n",
      " |  \n",
      " |  prod(...)\n",
      " |      prod(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.prod`\n",
      " |  \n",
      " |  pstrf(...)\n",
      " |      pstrf(upper=True, tol=-1) -> (Tensor, IntTensor)\n",
      " |      \n",
      " |      See :func:`torch.pstrf`\n",
      " |  \n",
      " |  put_(...)\n",
      " |      put_(indices, tensor, accumulate=False) -> Tensor\n",
      " |      \n",
      " |      Copies the elements from :attr:`tensor` into the positions specified by\n",
      " |      indices. For the purpose of indexing, the :attr:`self` tensor is treated as if\n",
      " |      it were a 1-D tensor.\n",
      " |      \n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`tensor` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if indices\n",
      " |      contain duplicate elements.\n",
      " |      \n",
      " |      Args:\n",
      " |          indices (LongTensor): the indices into self\n",
      " |          tensor (Tensor): the tensor containing values to copy from\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> src = torch.tensor([[4, 3, 5],\n",
      " |                                  [6, 7, 8]])\n",
      " |          >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\n",
      " |          tensor([[  4,   9,   5],\n",
      " |                  [ 10,   7,   8]])\n",
      " |  \n",
      " |  qr(...)\n",
      " |      qr() -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.qr`\n",
      " |  \n",
      " |  random_(...)\n",
      " |      random_(from=0, to=None, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers sampled from the discrete uniform\n",
      " |      distribution over ``[from, to - 1]``. If not specified, the values are usually\n",
      " |      only bounded by :attr:`self` tensor's data type. However, for floating point\n",
      " |      types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every\n",
      " |      value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`\n",
      " |      will be uniform in ``[0, 2^53]``.\n",
      " |  \n",
      " |  reciprocal(...)\n",
      " |      reciprocal() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.reciprocal`\n",
      " |  \n",
      " |  reciprocal_(...)\n",
      " |      reciprocal_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.reciprocal`\n",
      " |  \n",
      " |  record_stream(...)\n",
      " |  \n",
      " |  relu(...)\n",
      " |  \n",
      " |  relu_(...)\n",
      " |  \n",
      " |  remainder(...)\n",
      " |      remainder(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.remainder`\n",
      " |  \n",
      " |  remainder_(...)\n",
      " |      remainder_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.remainder`\n",
      " |  \n",
      " |  renorm(...)\n",
      " |      renorm(p, dim, maxnorm) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.renorm`\n",
      " |  \n",
      " |  renorm_(...)\n",
      " |      renorm_(p, dim, maxnorm) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.renorm`\n",
      " |  \n",
      " |  repeat(...)\n",
      " |      repeat(*sizes) -> Tensor\n",
      " |      \n",
      " |      Repeats this tensor along the specified dimensions.\n",
      " |      \n",
      " |      Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
      " |      \n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): The number of times to repeat this tensor along each\n",
      " |              dimension\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1, 2, 3])\n",
      " |          >>> x.repeat(4, 2)\n",
      " |          tensor([[ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3]])\n",
      " |          >>> x.repeat(4, 2, 1).size()\n",
      " |          torch.Size([4, 2, 3])\n",
      " |  \n",
      " |  requires_grad_(...)\n",
      " |      requires_grad_(requires_grad=True) -> Tensor\n",
      " |      \n",
      " |      Change if autograd should record operations on this tensor: sets this tensor's\n",
      " |      :attr:`requires_grad` attribute in-place. Returns this tensor.\n",
      " |      \n",
      " |      :func:`require_grad_`'s main use case is to tell autograd to begin recording\n",
      " |      operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False``\n",
      " |      (because it was obtained through a DataLoader, or required preprocessing or\n",
      " |      initialization), ``tensor.requires_grad_()`` makes it so that autograd will\n",
      " |      begin to record operations on ``tensor``.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): If autograd should record operations on this tensor.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # Let's say we want to preprocess some saved weights and use\n",
      " |          >>> # the result as new weights.\n",
      " |          >>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
      " |          >>> loaded_weights = torch.tensor(saved_weights)\n",
      " |          >>> weights = preprocess(loaded_weights)  # some function\n",
      " |          >>> weights\n",
      " |          tensor([-0.5503,  0.4926, -2.1158, -0.8303])\n",
      " |      \n",
      " |          >>> # Now, start to record operations done to weights\n",
      " |          >>> weights.requires_grad_()\n",
      " |          >>> out = weights.pow(2).sum()\n",
      " |          >>> out.backward()\n",
      " |          >>> weights.grad\n",
      " |          tensor([-1.1007,  0.9853, -4.2316, -1.6606])\n",
      " |  \n",
      " |  reshape(...)\n",
      " |      reshape(*shape) -> Tensor\n",
      " |      \n",
      " |      Returns a tensor with the same data and number of elements as :attr:`self`,\n",
      " |      but with the specified shape.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape (tuple of ints or int...): the desired shape\n",
      " |      \n",
      " |      See :func:`torch.reshape`\n",
      " |  \n",
      " |  resize_(...)\n",
      " |      resize_(*sizes) -> Tensor\n",
      " |      \n",
      " |      Resizes :attr:`self` tensor to the specified size. If the number of elements is\n",
      " |      larger than the current storage size, then the underlying storage is resized\n",
      " |      to fit the new number of elements. If the number of elements is smaller, the\n",
      " |      underlying storage is not changed. Existing elements are preserved but any new\n",
      " |      memory is uninitialized.\n",
      " |      \n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): the desired size\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
      " |          >>> x.resize_(2, 2)\n",
      " |          tensor([[ 1,  2],\n",
      " |                  [ 3,  4]])\n",
      " |  \n",
      " |  resize_as_(...)\n",
      " |      resize_as_(tensor) -> Tensor\n",
      " |      \n",
      " |      Resizes the :attr:`self` tensor to be the same size as the specified\n",
      " |      :attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``.\n",
      " |  \n",
      " |  rfft(...)\n",
      " |      rfft(signal_ndim, normalized=False, onesided=True) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.rfft`\n",
      " |  \n",
      " |  round(...)\n",
      " |      round() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.round`\n",
      " |  \n",
      " |  round_(...)\n",
      " |      round_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.round`\n",
      " |  \n",
      " |  rsqrt(...)\n",
      " |      rsqrt() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.rsqrt`\n",
      " |  \n",
      " |  rsqrt_(...)\n",
      " |      rsqrt_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.rsqrt`\n",
      " |  \n",
      " |  scatter_(...)\n",
      " |      scatter_(dim, index, src) -> Tensor\n",
      " |      \n",
      " |      Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
      " |      index is specified by its index in :attr:`src` for dimension != :attr:`dim` and\n",
      " |      by the corresponding value in :attr:`index` for dimension = :attr:`dim`.\n",
      " |      \n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |      \n",
      " |          self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
      " |      \n",
      " |      This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
      " |      \n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should have same number of\n",
      " |      dimensions. It is also required that `index->size[d] <= src->size[d]` for all\n",
      " |      dimension `d`, and that `index->size[d] <= real->size[d]` for all dimensions\n",
      " |      `d != dim`.\n",
      " |      \n",
      " |      Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
      " |      between `0` and `(self.size(dim) -1)` inclusive, and all values in a row along\n",
      " |      the specified dimension :attr:`dim` must be unique.\n",
      " |      \n",
      " |      Args:\n",
      " |          input (Tensor): the source tensor\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter\n",
      " |          src (Tensor or float): the source element(s) to scatter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.rand(2, 5)\n",
      " |          >>> x\n",
      " |          tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],\n",
      " |                  [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])\n",
      " |          >>> torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\n",
      " |          tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],\n",
      " |                  [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],\n",
      " |                  [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])\n",
      " |      \n",
      " |          >>> z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)\n",
      " |          >>> z\n",
      " |          tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],\n",
      " |                  [ 0.0000,  0.0000,  0.0000,  1.2300]])\n",
      " |  \n",
      " |  scatter_add_(...)\n",
      " |  \n",
      " |  select(...)\n",
      " |      select(dim, index) -> Tensor\n",
      " |      \n",
      " |      Slices the :attr:`self` tensor along the selected dimension at the given index.\n",
      " |      This function returns a tensor with the given dimension removed.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): the dimension to slice\n",
      " |          index (int): the index to select with\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          :meth:`select` is equivalent to slicing. For example,\n",
      " |          ``tensor.select(0, index)`` is equivalent to ``tensor[index]`` and\n",
      " |          ``tensor.select(2, index)`` is equivalent to ``tensor[:,:,index]``.\n",
      " |  \n",
      " |  set_(...)\n",
      " |      set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
      " |      \n",
      " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      " |      :attr:`self` tensor will share the same storage and have the same size and\n",
      " |      strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
      " |      in the other.\n",
      " |      \n",
      " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      " |      storage, offset, size, and stride.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (Tensor or Storage): the tensor or storage to use\n",
      " |          storage_offset (int, optional): the offset in the storage\n",
      " |          size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
      " |          stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
      " |  \n",
      " |  short(...)\n",
      " |      short() -> Tensor\n",
      " |      \n",
      " |      ``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.\n",
      " |  \n",
      " |  sigmoid(...)\n",
      " |      sigmoid() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sigmoid`\n",
      " |  \n",
      " |  sigmoid_(...)\n",
      " |      sigmoid_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sigmoid`\n",
      " |  \n",
      " |  sign(...)\n",
      " |      sign() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sign`\n",
      " |  \n",
      " |  sign_(...)\n",
      " |      sign_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sign`\n",
      " |  \n",
      " |  sin(...)\n",
      " |      sin() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sin`\n",
      " |  \n",
      " |  sin_(...)\n",
      " |      sin_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sin`\n",
      " |  \n",
      " |  sinh(...)\n",
      " |      sinh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sinh`\n",
      " |  \n",
      " |  sinh_(...)\n",
      " |      sinh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sinh`\n",
      " |  \n",
      " |  size(...)\n",
      " |      size() -> torch.Size\n",
      " |      \n",
      " |      Returns the size of the :attr:`self` tensor. The returned value is a subclass of\n",
      " |      :class:`tuple`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> torch.empty(3, 4, 5).size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |  \n",
      " |  slice(...)\n",
      " |  \n",
      " |  slogdet(...)\n",
      " |      slogdet() -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.slogdet`\n",
      " |  \n",
      " |  smm(...)\n",
      " |  \n",
      " |  sort(...)\n",
      " |      sort(dim=None, descending=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.sort`\n",
      " |  \n",
      " |  split_with_sizes(...)\n",
      " |  \n",
      " |  sqrt(...)\n",
      " |      sqrt() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sqrt`\n",
      " |  \n",
      " |  sqrt_(...)\n",
      " |      sqrt_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sqrt`\n",
      " |  \n",
      " |  squeeze(...)\n",
      " |      squeeze(dim=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.squeeze`\n",
      " |  \n",
      " |  squeeze_(...)\n",
      " |      squeeze_(dim=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.squeeze`\n",
      " |  \n",
      " |  sspaddmm(...)\n",
      " |  \n",
      " |  std(...)\n",
      " |      std(dim=None, unbiased=True, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.std`\n",
      " |  \n",
      " |  stft(...)\n",
      " |      stft(frame_length, hop, fft_size=None, return_onesided=True, window=None, pad_end=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.stft`\n",
      " |  \n",
      " |  storage(...)\n",
      " |      storage() -> torch.Storage\n",
      " |      \n",
      " |      Returns the underlying storage\n",
      " |  \n",
      " |  storage_offset(...)\n",
      " |      storage_offset() -> int\n",
      " |      \n",
      " |      Returns :attr:`self` tensor's offset in the underlying storage in terms of\n",
      " |      number of storage elements (not bytes).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1, 2, 3, 4, 5])\n",
      " |          >>> x.storage_offset()\n",
      " |          0\n",
      " |          >>> x[3:].storage_offset()\n",
      " |          3\n",
      " |  \n",
      " |  storage_type(...)\n",
      " |  \n",
      " |  stride(...)\n",
      " |      stride(dim) -> tuple or int\n",
      " |      \n",
      " |      Returns the stride of :attr:`self` tensor.\n",
      " |      \n",
      " |      Stride is the jump necessary to go from one element to the next one in the\n",
      " |      specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
      " |      argument is passed in. Otherwise, an integer value is returned as the stride in\n",
      " |      the particular dimension :attr:`dim`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int, optional): the desired dimension in which stride is required\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
      " |          >>> x.stride()\n",
      " |          (5, 1)\n",
      " |          >>>x.stride(0)\n",
      " |          5\n",
      " |          >>> x.stride(-1)\n",
      " |          1\n",
      " |  \n",
      " |  sub(...)\n",
      " |      sub(value, other) -> Tensor\n",
      " |      \n",
      " |      Subtracts a scalar or tensor from :attr:`self` tensor. If both :attr:`value` and\n",
      " |      :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
      " |      :attr:`value` before being used.\n",
      " |      \n",
      " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |  \n",
      " |  sub_(...)\n",
      " |      sub_(x) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sub`\n",
      " |  \n",
      " |  sum(...)\n",
      " |      sum(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sum`\n",
      " |  \n",
      " |  svd(...)\n",
      " |      svd(some=True) -> (Tensor, Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.svd`\n",
      " |  \n",
      " |  symeig(...)\n",
      " |      symeig(eigenvectors=False, upper=True) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.symeig`\n",
      " |  \n",
      " |  t(...)\n",
      " |      t() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.t`\n",
      " |  \n",
      " |  t_(...)\n",
      " |      t_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.t`\n",
      " |  \n",
      " |  take(...)\n",
      " |      take(indices) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.take`\n",
      " |  \n",
      " |  tan(...)\n",
      " |  \n",
      " |  tan_(...)\n",
      " |      tan_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tan`\n",
      " |  \n",
      " |  tanh(...)\n",
      " |      tanh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tanh`\n",
      " |  \n",
      " |  tanh_(...)\n",
      " |      tanh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tanh`\n",
      " |  \n",
      " |  to(...)\n",
      " |      to(*args, **kwargs) -> Tensor\n",
      " |      \n",
      " |      Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are\n",
      " |      inferred from the arguments of ``self.to(*args, **kwargs)``.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``self`` Tensor already\n",
      " |          has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.\n",
      " |          Otherwise, the returned tensor is a copy of ``self`` with the desired\n",
      " |          :class:`torch.dtype` and :class:`torch.device`.\n",
      " |      \n",
      " |      Here are the ways to call ``to``:\n",
      " |      \n",
      " |      .. function:: to(dtype) -> Tensor\n",
      " |      \n",
      " |          Returns a Tensor with the specified :attr:`dtype`\n",
      " |      \n",
      " |      .. function:: to(device, dtype=None) -> Tensor\n",
      " |      \n",
      " |          Returns a Tensor with the specified :attr:`device` and (optional)\n",
      " |          :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.\n",
      " |      \n",
      " |      .. function:: to(other) -> Tensor\n",
      " |      \n",
      " |          Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as the Tensor\n",
      " |          :attr:`other`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n",
      " |          >>> tensor.to(torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64)\n",
      " |      \n",
      " |          >>> cuda0 = torch.device('cuda:0')\n",
      " |          >>> tensor.to(cuda0)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], device='cuda:0')\n",
      " |      \n",
      " |          >>> tensor.to(cuda0, dtype=torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |      \n",
      " |          >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n",
      " |          >>> tensor.to(other)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |  \n",
      " |  to_dense(...)\n",
      " |  \n",
      " |  tolist(...)\n",
      " |  \n",
      " |  topk(...)\n",
      " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.topk`\n",
      " |  \n",
      " |  trace(...)\n",
      " |      trace() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.trace`\n",
      " |  \n",
      " |  transpose(...)\n",
      " |      transpose(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.transpose`\n",
      " |  \n",
      " |  transpose_(...)\n",
      " |      transpose_(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.transpose`\n",
      " |  \n",
      " |  tril(...)\n",
      " |      tril(k=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tril`\n",
      " |  \n",
      " |  tril_(...)\n",
      " |      tril_(k=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tril`\n",
      " |  \n",
      " |  triu(...)\n",
      " |      triu(k=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.triu`\n",
      " |  \n",
      " |  triu_(...)\n",
      " |      triu_(k=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.triu`\n",
      " |  \n",
      " |  trtrs(...)\n",
      " |      trtrs(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.trtrs`\n",
      " |  \n",
      " |  trunc(...)\n",
      " |      trunc() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.trunc`\n",
      " |  \n",
      " |  trunc_(...)\n",
      " |      trunc_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.trunc`\n",
      " |  \n",
      " |  type(...)\n",
      " |      type(dtype=None, non_blocking=False, **kwargs) -> str or Tensor\n",
      " |      Returns the type if `dtype` is not provided, else casts this object to\n",
      " |      the specified type.\n",
      " |      \n",
      " |      If this is already of the correct type, no copy is performed and the\n",
      " |      original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          dtype (type or string): The desired type\n",
      " |          non_blocking (bool): If ``True``, and the source is in pinned memory\n",
      " |              and destination is on the GPU or vice versa, the copy is performed\n",
      " |              asynchronously with respect to the host. Otherwise, the argument\n",
      " |              has no effect.\n",
      " |          **kwargs: For compatibility, may contain the key ``async`` in place of\n",
      " |              the ``non_blocking`` argument. The ``async`` arg is deprecated.\n",
      " |  \n",
      " |  type_as(...)\n",
      " |      type_as(tensor) -> Tensor\n",
      " |      \n",
      " |      Returns this tensor cast to the type of the given tensor.\n",
      " |      \n",
      " |      This is a no-op if the tensor is already of the correct type. This is\n",
      " |      equivalent to::\n",
      " |      \n",
      " |          self.type(tensor.type())\n",
      " |      \n",
      " |      Params:\n",
      " |          tensor (Tensor): the tensor which has the desired type\n",
      " |  \n",
      " |  unfold(...)\n",
      " |      unfold(dim, size, step) -> Tensor\n",
      " |      \n",
      " |      Returns a tensor which contains all slices of size :attr:`size` from\n",
      " |      :attr:`self` tensor in the dimension :attr:`dim`.\n",
      " |      \n",
      " |      Step between two slices is given by :attr:`step`.\n",
      " |      \n",
      " |      If `sizedim` is the size of dimension dim for :attr:`self`, the size of\n",
      " |      dimension :attr:`dim` in the returned tensor will be\n",
      " |      `(sizedim - size) / step + 1`.\n",
      " |      \n",
      " |      An additional dimension of size size is appended in the returned tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension in which unfolding happens\n",
      " |          size (int): the size of each slice that is unfolded\n",
      " |          step (int): the step between each slice\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.arange(1, 8)\n",
      " |          >>> x\n",
      " |          tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n",
      " |          >>> x.unfold(0, 2, 1)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 2.,  3.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 4.,  5.],\n",
      " |                  [ 5.,  6.],\n",
      " |                  [ 6.,  7.]])\n",
      " |          >>> x.unfold(0, 2, 2)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 5.,  6.]])\n",
      " |  \n",
      " |  uniform_(...)\n",
      " |      uniform_(from=0, to=1) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers sampled from the continuous uniform\n",
      " |      distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |          P(x) = \\dfrac{1}{\\text{to} - \\text{from}}\n",
      " |  \n",
      " |  unsqueeze(...)\n",
      " |      unsqueeze(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.unsqueeze`\n",
      " |  \n",
      " |  unsqueeze_(...)\n",
      " |      unsqueeze_(dim) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
      " |  \n",
      " |  var(...)\n",
      " |      var(dim=None, unbiased=True, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.var`\n",
      " |  \n",
      " |  view(...)\n",
      " |      view(*args) -> Tensor\n",
      " |      \n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different size.\n",
      " |      \n",
      " |      The returned tensor shares the same data and must have the same number\n",
      " |      of elements, but may have a different size. For a tensor to be viewed, the new\n",
      " |      view size must be compatible with its original size and stride, i.e., each new\n",
      " |      view dimension must either be a subspace of an original dimension, or only span\n",
      " |      across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      " |      contiguity-like condition that :math:`\\forall i = 0, \\dots, k-1`,\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |        stride[i] = stride[i+1] \\times size[i+1]\n",
      " |      \n",
      " |      Otherwise, :func:`contiguous` needs to be called before the tensor can be\n",
      " |      viewed.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (torch.Size or int...): the desired size\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([4, 4])\n",
      " |          >>> y = x.view(16)\n",
      " |          >>> y.size()\n",
      " |          torch.Size([16])\n",
      " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      " |          >>> z.size()\n",
      " |          torch.Size([2, 8])\n",
      " |  \n",
      " |  where(...)\n",
      " |      where(condition, y) -> Tensor\n",
      " |      \n",
      " |      ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.\n",
      " |      See :func:`torch.where`\n",
      " |  \n",
      " |  zero_(...)\n",
      " |      zero_() -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C._TensorBase:\n",
      " |  \n",
      " |  data\n",
      " |  \n",
      " |  device\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  grad\n",
      " |  \n",
      " |  grad_fn\n",
      " |  \n",
      " |  is_cuda\n",
      " |  \n",
      " |  is_leaf\n",
      " |  \n",
      " |  is_sparse\n",
      " |  \n",
      " |  layout\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  output_nr\n",
      " |  \n",
      " |  requires_grad\n",
      " |  \n",
      " |  shape\n",
      " |  \n",
      " |  volatile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(w,b)\n",
    "#help(w)\n",
    "print(x_train,y_train)\n",
    "help(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建线性回归模型\n",
    "x_train = Variable(x_train)\n",
    "y_train = Variable(y_train)\n",
    "\n",
    "def linear_model(x):\n",
    "    return x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = linear_model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上面的步骤我们就定义好了模型，在进行参数更新之前，我们可以先看看模型的输出结果长什么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x111e19da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')\n",
    "plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考：红色的点表示预测值，似乎排列成一条直线，请思考一下这些点是否在一条直线上？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候需要计算我们的误差函数，也就是\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n(\\hat{y}_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算误差\n",
    "def get_loss(y_, y):\n",
    "    return torch.mean((y_ - y_train) ** 2)\n",
    "\n",
    "loss = get_loss(y_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.3156)\n"
     ]
    }
   ],
   "source": [
    "# 打印一下看看 loss 的大小\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好了误差函数，接下来我们需要计算 w 和 b 的梯度了，这时得益于 PyTorch 的自动求导，我们不需要手动去算梯度，有兴趣的同学可以手动计算一下，w 和 b 的梯度分别是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^n x_i(w x_i + b - y_i) \\\\\n",
    "\\frac{\\partial}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动求导\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [ 3.2121])\n",
      "tensor([-0.2235])\n"
     ]
    }
   ],
   "source": [
    "# 查看 w 和 b 的梯度\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新一次参数\n",
    "w.data = w.data - 1e-2 * w.grad.data\n",
    "b.data = b.data - 1e-2 * b.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新完成参数之后，我们再一次看看模型输出的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11207cef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_ = linear_model(x_train)\n",
    "plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')\n",
    "plt.plot(x_train.data.numpy(), y_.data.numpy(), 'ro', label='estimated')\n",
    "y_ = linear_model(x_train)\n",
    "plt.plot(x_train.data.numpy(), y_.data.numpy(), 'g+', label='estimated2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子可以看到，更新之后红色的线跑到了蓝色的线下面，没有特别好的拟合蓝色的真实值，所以我们需要在进行几次更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.106460690498352\n",
      "epoch: 1, loss: 0.28777116537094116\n",
      "epoch: 2, loss: 0.27209919691085815\n",
      "epoch: 3, loss: 0.2712911069393158\n",
      "epoch: 4, loss: 0.27076077461242676\n",
      "epoch: 5, loss: 0.2702382206916809\n",
      "epoch: 6, loss: 0.2697184979915619\n",
      "epoch: 7, loss: 0.2692013382911682\n",
      "epoch: 8, loss: 0.26868686079978943\n",
      "epoch: 9, loss: 0.26817506551742554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "for e in range(10): # 进行 10 次更新\n",
    "    y_ = linear_model(x_train)\n",
    "    loss = get_loss(y_, y_train)\n",
    "    \n",
    "    w.grad.zero_() # 记得归零梯度\n",
    "    b.grad.zero_() # 记得归零梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    w.data = w.data - 1e-2 * w.grad.data # 更新 w\n",
    "    b.data = b.data - 1e-2 * b.grad.data # 更新 b \n",
    "    print('epoch: {}, loss: {}'.format(e, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (61) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9b3708fdc201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'estimated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-af37c007fbb1>\u001b[0m in \u001b[0;36mlinear_model\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (61) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "y_ = linear_model(x_train)\n",
    "plt.plot(x_train.data.numpy(), y_train.data.numpy(), 'bo', label='real')\n",
    "plt.plot(x_train.data.numpy(), y_.data.numpy(), 'r', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过 10 次更新，我们发现红色的预测结果已经比较好的拟合了蓝色的真实值。\n",
    "\n",
    "现在你已经学会了你的第一个机器学习模型了，再接再厉，完成下面的小练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：**\n",
    "\n",
    "重启 notebook 运行上面的线性回归模型，但是改变训练次数以及不同的学习率进行尝试得到不同的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们更进一步，讲一讲多项式回归。什么是多项式回归呢？非常简单，根据上面的线性回归模型\n",
    "\n",
    "$$\n",
    "\\hat{y} = w x + b\n",
    "$$\n",
    "\n",
    "这里是关于 x 的一个一次多项式，这个模型比较简单，没有办法拟合比较复杂的模型，所以我们可以使用更高次的模型，比如\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots\n",
    "$$\n",
    "\n",
    "这样就能够拟合更加复杂的模型，这就是多项式模型，这里使用了 x 的更高次，同理还有多元回归模型，形式也是一样的，只是出了使用 x，还是更多的变量，比如 y、z 等等，同时他们的 loss 函数和简单的线性回归模型是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们可以先定义一个需要拟合的目标函数，这个函数是个三次的多项式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0.90 + 0.50 * x + 3.00 * x^2 + 2.40 * x^3\n"
     ]
    }
   ],
   "source": [
    "# 定义一个多变量函数\n",
    "\n",
    "w_target = np.array([0.5, 3, 2.4]) # 定义参数\n",
    "b_target = np.array([0.9]) # 定义参数\n",
    "\n",
    "f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(\n",
    "    b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的式子\n",
    "\n",
    "print(f_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以先画出这个多项式的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1126dc128>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出这个函数的曲线\n",
    "x_sample = np.arange(-3, 3.1, 0.1)\n",
    "y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3\n",
    "\n",
    "plt.plot(x_sample, y_sample,'bo', label='real curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们可以构建数据集，需要 x 和 y，同时是一个三次多项式，所以我们取了 $x,\\ x^2, x^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据 x 和 y\n",
    "# x 是一个如下矩阵 [x, x^2, x^3]\n",
    "# y 是函数的结果 [y]\n",
    "\n",
    "x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)\n",
    "x_train = torch.from_numpy(x_train).float() # 转换成 float tensor\n",
    "\n",
    "y_train = torch.from_numpy(y_sample).float().unsqueeze(1) # 转化成 float tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们可以定义需要优化的参数，就是前面这个函数里面的 $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数和模型\n",
    "w = Variable(torch.randn(3, 1), requires_grad=True)\n",
    "b = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "# 将 x 和 y 转换成 Variable\n",
    "x_train = Variable(x_train)\n",
    "y_train = Variable(y_train)\n",
    "\n",
    "def multi_linear(x):\n",
    "    return torch.mm(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以画出没有更新之前的模型和真实的模型之间的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x112833780>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出更新之前的模型\n",
    "y_pred = multi_linear(x_train)\n",
    "\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_sample,'bo', label='real curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，这两条曲线之间存在差异，我们计算一下他们之间的误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1221.2911)\n"
     ]
    }
   ],
   "source": [
    "# 计算误差，这里的误差和一元的线性模型的误差是相同的，前面已经定义过了 get_loss\n",
    "loss = get_loss(y_pred, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动求导\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -97.1203],\n",
      "        [-147.9059],\n",
      "        [-644.4846]])\n",
      "tensor([-27.3156])\n"
     ]
    }
   ],
   "source": [
    "# 查看一下 w 和 b 的梯度\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新一下参数\n",
    "w.data = w.data - 0.001 * w.grad.data\n",
    "b.data = b.data - 0.001 * b.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1125f5278>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出更新一次之后的模型\n",
    "y_pred = multi_linear(x_train)\n",
    "\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_sample,'bo', label='real curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为只更新了一次，所以两条曲线之间的差异仍然存在，我们进行 100 次迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, Loss: 73.99384\n",
      "epoch 40, Loss: 17.32101\n",
      "epoch 60, Loss: 4.09729\n",
      "epoch 80, Loss: 1.00907\n",
      "epoch 100, Loss: 0.28636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 进行 100 次参数更新\n",
    "for e in range(100):\n",
    "    y_pred = multi_linear(x_train)\n",
    "    loss = get_loss(y_pred, y_train)\n",
    "    \n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.data = w.data - 0.001 * w.grad.data\n",
    "    b.data = b.data - 0.001 * b.grad.data\n",
    "    if (e + 1) % 20 == 0:\n",
    "        print('epoch {}, Loss: {:.5f}'.format(e+1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到更新完成之后 loss 已经非常小了，我们画出更新之后的曲线对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11344f2b0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出更新之后的结果\n",
    "y_pred = multi_linear(x_train)\n",
    "\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve', color='r')\n",
    "plt.plot(x_train.data.numpy()[:, 0], y_sample,'bo', label='real curve', color='b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，经过 100 次更新之后，可以看到拟合的线和真实的线已经完全重合了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**小练习：上面的例子是一个三次的多项式，尝试使用二次的多项式去拟合它，看看最后能做到多好**\n",
    "\n",
    "**提示：参数 `w = torch.randn(2, 1)`，同时重新构建 x 数据集**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
